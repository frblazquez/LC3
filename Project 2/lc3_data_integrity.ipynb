{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LC3 DATA INTEGRITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for general data management\n",
    "import pandas as pd\n",
    "import numpy  as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas display options customization\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 12)\n",
    "#pd.set_option('display.width', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file path (can be also an excel)\n",
    "DATA_MERGED_PATH = './data/data_merged.ods'\n",
    "DATA_FULL_PATH   = './data/data_full.ods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read merged data skipping the first row and considering also '-' as NaN\n",
    "data_merged = pd.read_excel(DATA_MERGED_PATH, skiprows=[0], na_values=['-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read full data and remove empty lines\n",
    "data_full = pd.read_excel(DATA_FULL_PATH,sheet_name='Clays_CS')\n",
    "data_full.dropna(how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mean of measurements for each clay\n",
    "# data_full.groupby('Clay').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Std of measurements for each clay\n",
    "# data_full.groupby('Clay').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements for each clay and each day/total for each day\n",
    "# data_full.groupby('Clay').count()\n",
    "# data_full.groupby('Clay').count().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points that fails in mean coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have to look at the excel of the means with some suspicion, it is enough to get an idea, but not to take it as perfectly correct \n",
    "* B45 India 1, B45 India 3, Loma Sur, are very different in their average and std's every day\n",
    "* Argex, Chile,Iran, it was tolerable(~1)\n",
    "* Iran in day28 it seems to give a little more problems, but I insist on the idea that the excel of the averages is not reliable, and if we take into account that the average of only 2 measures does not give us as much security as in the case where the average would have been done with 4 or 5 measures, so I decide to leave it. For example in Iran G1 one of the measures is 56.06875 and the other 51.56875, so the average is somewhere in between these 2, if we had more measures it would give us more certainty. I have also checked in the big table that the values were well placed.\n",
    "* Chile well copied values. Chile day28 its similar to Iran, we have 2 measures 58,8375 and 54,98125. the 7th and 90th also have two measurements and the average of only two measurements is not very rigorous either\n",
    "* Argex day28 well copied values. Only 2 meassures.\n",
    "* ChinaSCreened only fails on day 1, I have checked that the values were well copied. It is not a very very big difference \n",
    "\n",
    "* Holcim 4 Brazil, habia que tener cuidado, porque se hicieron 2 experimentos, hay Holcim 4 2, y estaba comparondolo con esos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEAN_DEVIATION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_mean = data_merged[['Clay','1D','3D','7D','28D','90D']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_check = pd.merge(data_full.groupby('Clay').mean(), data_merged_mean, on='Clay', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_check[abs(mean_check['day_1'] - mean_check['1D']) > MAX_MEAN_DEVIATION][['Clay','day_1','1D']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_check[abs(mean_check['day_3'] - mean_check['3D']) > MAX_MEAN_DEVIATION][['Clay','day_3','3D']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_check[abs(mean_check['day_7'] - mean_check['7D']) > MAX_MEAN_DEVIATION][['Clay','day_7','7D']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_check[abs(mean_check['day_28'] - mean_check['28D']) > MAX_MEAN_DEVIATION][['Clay','day_28','28D']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_check[abs(mean_check['day_90'] - mean_check['90D']) > MAX_MEAN_DEVIATION][['Clay','day_90','90D']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points that fails in std coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only fails on day 7 F1 y on day 90 India 2. They can be errors of the average excel perfectly, I leave it as correct, because in the rest of the days there is no problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STD_DEVIATION = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_std = data_merged[['Clay','STD','STD.1','STD.2','STD.3','STD.4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_check = pd.merge(data_full.groupby('Clay').std(), data_merged_std, on='Clay', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_check[abs(std_check['day_1'] - std_check['STD']) > MAX_STD_DEVIATION][['Clay','day_1','STD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_check[abs(std_check['day_3'] - std_check['STD.1']) > MAX_STD_DEVIATION][['Clay','day_3','STD.1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "std_check[abs(std_check['day_7'] - std_check['STD.2']) > MAX_STD_DEVIATION][['Clay','day_7','STD.2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_check[abs(std_check['day_28'] - std_check['STD.3']) > MAX_STD_DEVIATION][['Clay','day_28','STD.3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_check[abs(std_check['day_90'] - std_check['STD.4']) > MAX_STD_DEVIATION][['Clay','day_90','STD.4']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
