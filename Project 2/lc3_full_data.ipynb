{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LC3 DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we analize the relation of the compressive strength of LC3 with the clay used for its elaboration properties. For this purpose we utilize the dataset given by the Constructions Materials Laboratory at EPFL. This dataset consists of different measures of compressive strength for different types of LC3 cement in which the clay used for their ellaboration varies. We have also several clay properties measured and the objetive is finding the relation of the compressive strength (CS) with these for a deeper understanding of key elements in LC3 cement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lc3_implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file path (can be also an excel)\n",
    "DATA_MERGED_PATH = './data/data_merged.ods'\n",
    "DATA_FULL_PATH   = './data/data_full.ods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read full data and remove empty lines\n",
    "data_full = pd.read_excel(DATA_FULL_PATH,sheet_name='Clays_CS')\n",
    "data_full.dropna(how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read clay properties\n",
    "data_clay   = pd.read_excel(DATA_FULL_PATH,sheet_name='Clays_properties')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to have the whole dataset\n",
    "data_full_clay = pd.merge(data_full, data_clay, left_on='Clay', right_on='Clay', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rename some columns for having an easier reference\n",
    "data_full_clay.rename(columns = {'Calcined kaolinite content (%)':'Kaolinite_content'}, inplace = True)\n",
    "data_full_clay.rename(columns = {'Dv,50 (µm)':'Dv50'                                 }, inplace = True)\n",
    "data_full_clay.rename(columns = {'BET Specific surface (m2/g)':'BET_specific_surface'}, inplace = True)\n",
    "\n",
    "data_full_clay.rename(columns = {'STD'  : 'STD_1D'}, inplace = True)\n",
    "data_full_clay.rename(columns = {'STD.1': 'STD_3D'}, inplace = True)\n",
    "data_full_clay.rename(columns = {'STD.2': 'STD_7D'}, inplace = True)\n",
    "data_full_clay.rename(columns = {'STD.3':'STD_28D'}, inplace = True)\n",
    "data_full_clay.rename(columns = {'STD.4':'STD_90D'}, inplace = True)\n",
    "\n",
    "# Sorting allows us to plot functions more easily\n",
    "data_full_clay = data_full_clay.sort_values('Kaolinite_content')\n",
    "\n",
    "# Get useful dataframe information\n",
    "#data               # Get data\n",
    "#data.describe()    # Get data general information\n",
    "#data.columns       # Get data features names\n",
    "#data.corr()        # Get correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take kaolinite content\n",
    "x = data_full_clay['Kaolinite_content'].values\n",
    "\n",
    "# Take compression strengths\n",
    "y1  = data_full_clay[ \"day_1\"].values\n",
    "y3  = data_full_clay[ \"day_3\"].values\n",
    "y7  = data_full_clay[ \"day_7\"].values\n",
    "y28 = data_full_clay[\"day_28\"].values\n",
    "y90 = data_full_clay[\"day_90\"].values\n",
    "\n",
    "# Measures have missing values\n",
    "x1 =   x[np.logical_not(np.isnan(y1))]\n",
    "y1 = y1[np.logical_not(np.isnan(y1))]\n",
    "x3 =   x[np.logical_not(np.isnan(y3))]\n",
    "y3 = y3[np.logical_not(np.isnan(y3))]\n",
    "x7 =   x[np.logical_not(np.isnan(y7))]\n",
    "y7 = y7[np.logical_not(np.isnan(y7))]\n",
    "x28 =   x[np.logical_not(np.isnan(y28))]\n",
    "y28 = y28[np.logical_not(np.isnan(y28))]\n",
    "x90 =   x[np.logical_not(np.isnan(y90))]\n",
    "y90 = y90[np.logical_not(np.isnan(y90))]\n",
    "\n",
    "# Take standard deviations\n",
    "#z1  = data[ \"STD_1D\"].values\n",
    "#z3  = data[ \"STD_3D\"].values\n",
    "#z7  = data[ \"STD_7D\"].values\n",
    "#z28 = data[\"STD_28D\"].values\n",
    "#z90 = data[\"STD_90D\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show points using matplotlib.pyplot library\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plt.plot(x1,y1,'c^',x3,y3,'bs',x7,y7,'r^',x28,y28,'go', x90,y90,'m^' )\n",
    "plt.xlabel('%Kaolinite Content')\n",
    "plt.ylabel('Compressive Strenght')\n",
    "\n",
    "d1_patch  = mpatches.Patch(color='cyan',      label='After  1 day')\n",
    "d3_patch  = mpatches.Patch(color='blue',      label='After  3 days')\n",
    "d7_patch  = mpatches.Patch(color='red',       label='After  7 days')\n",
    "d28_patch = mpatches.Patch(color='darkgreen', label='After 28 days')\n",
    "d90_patch = mpatches.Patch(color='purple',    label='After 90 days')\n",
    "plt.legend(handles=[d1_patch,d3_patch,d7_patch,d28_patch,d90_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPRESSION STRENGTH (CS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the kaolinite content is the most predictive feature that we have. We are going to start creating simple linear regression models and then, following the appreciations obtained by visualizing the data, we are going to create non-linear models based on the kaolinite content for better fitting the data distribution as well as models with more features for avoiding data sparsification.\n",
    "\n",
    "Two metrics are going to be extremely important here:\n",
    "* **R square:** Is giving us a measurement of how good is our model (the closer to 1.0 the better). \n",
    "* **Validation score:** Is going to let us control overfitting. Improving R² means nothing if validation is worse. We'll use mean squared error with Leave One Out cross validation to estimate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove outlier in y1 data?\n",
    "# TODO: Idea for the report, removing pesimist outliers is not a good idea in our project!\n",
    "# TODO: Add OPC compression strength values for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear models based on the kaolinite content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "leave_one_out_validation(x1.reshape(-1,1), y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_out_validation(x3.reshape(-1,1), y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "leave_one_out_validation(x7.reshape(-1,1), y7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_out_validation(x28.reshape(-1,1), y28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "leave_one_out_validation(x90.reshape(-1,1), y90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First sight analysis:**\n",
    "\n",
    "* There is few more we can improve at the 7th day using only the kaolinite content, data distribution is quite a straight line.\n",
    "* For 1st and 3rd day the problem is more the sparsification of the points than the lack of expresivity of the model.\n",
    "* For 28th and 90th day until 40% of kaolinite content the compression strength increases linearly and then estabilizes. Makes sense a non-linear model.\n",
    "* It doesn't make sense in any model a degree 3 regression model, compression strength increases with kaolinite content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear models based on the kaolinite content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_2 = Kaolinite content, (Kaolinite content)^2\n",
    "x1_2   = np.array([x1, x1**2]).T\n",
    "x3_2   = np.array([x3, x3**2]).T\n",
    "x7_2   = np.array([x7, x7**2]).T\n",
    "x28_2   = np.array([x28, x28**2]).T\n",
    "x90_2 = np.array([x90,x90*x90]).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "leave_one_out_validation(x1_2, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_out_validation(x3_2, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_out_validation(x7_2, y7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "leave_one_out_validation(x28_2, y28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "leave_one_out_validation(x90_2, y90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First sight analysis:**\n",
    "\n",
    "* Expected results, better models obtained for 28th and 90th day compression strength obtained.\n",
    "* We might be experiencing overfitting with this model for 1st and 3rd day measurements because we're not increasing the compressive strength with the increase of calonita for small contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Dummy issue, we have generalized feature selection function but in one dataset days ar \"7D\" and in the other days are \"day_7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features are realiable? Those with more than RELIABLE_THRESHOLD measurements\n",
    "RELIABLE_THRESHOLD = 45\n",
    "\n",
    "features = data_full_clay.columns[14:]\n",
    "reliable_features = [f for f in features if data_full_clay[f].describe()[0] >= RELIABLE_THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the reliable features, with enough points to rely\n",
    "#reliable_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_selection(data_full_clay,reliable_features)\n",
    "#feature_selection(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression models based on the kaolinite content and other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the results obtained in the feature selection part, here we are creating and analyzing the models done with the kaolinite conent (in degree one and two) as well as other feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Same issue than in feature selection, unify days names in both datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Day 1 \n",
    "mod = smf.ols(formula='day_1 ~ Kaolinite_content + Kaolinite_content_square + Dv50', data=get_model_data(data_full_clay,'Dv50','1D'))\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1 \n",
    "mod = smf.ols(formula='day_1 ~ Kaolinite_content + Kaolinite_content_square + CaO', data=get_model_data('CaO','1D'))\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 3\n",
    "mod = smf.ols(formula='day_3 ~ Kaolinite_content + Kaolinite_content_square + Dv50', data=get_model_data('Dv50','3D'))\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 3\n",
    "mod = smf.ols(formula='day_3 ~ Kaolinite_content + Kaolinite_content_square + CaO', data=get_model_data('CaO','3D'))\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 7\n",
    "mod = smf.ols(formula='day_7 ~ Kaolinite_content + Kaolinite_content_square + BET_specific_surface', data=get_model_data('BET_specific_surface','7D'))\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 28\n",
    "mod = smf.ols(formula='day_2 ~ Kaolinite_content + Kaolinite_content_square + BET_specific_surface', data=get_model_data('BET_specific_surface','28D'))\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 28\n",
    "mod = smf.ols(formula='day_2 ~ Kaolinite_content + Kaolinite_content_square + TiO2', data=get_model_data('TiO2','28D'))\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 90\n",
    "mod = smf.ols(formula='day_9 ~ Kaolinite_content + Kaolinite_content_square + BET_specific_surface', data=get_model_data('BET_specific_surface','90D'))\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Day 90\n",
    "mod = smf.ols(formula='day_9 ~ Kaolinite_content + Kaolinite_content_square + TiO2', data=get_model_data('TiO2','90D'))\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence analysis for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know our model (function depending on the features provided) as well as some metrics to have an idea of how well our model fits our data (R2) and how is it behaving in practice with new data (MSE). The objetive of this section is to provide a more mathematical analysis of the confidence we can expect from our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy example for putting things into practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with the Kaolinite content based model for compression strength at day 90 to exemplify the tools and techniques we can use to estimate confidence intervals in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_90=data_full_clay[['day_90','Kaolinite_content']].dropna().copy()\n",
    "data_90.insert(2, 'Kaolinite_content_square', data_90['Kaolinite_content']**2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Day 90\n",
    "mod = smf.ols(formula='day_90 ~ Kaolinite_content_square + Kaolinite_content', data=data_90)\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_intervals(data_90, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prstd, iv_l, iv_u = wls_prediction_std(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.plot(x90, y90, 'o', label=\"data\")\n",
    "#ax.plot(x90, model.predict(x90.reshape(-1,1)), 'b-', label=\"True\")\n",
    "ax.plot(x90, res.fittedvalues, 'r--.', label=\"OLS\")\n",
    "ax.plot(x90, iv_u, 'r--')\n",
    "ax.plot(x90, iv_l, 'r--')\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_28=data_full_clay[['day_28','Kaolinite_content']].dropna().copy()\n",
    "data_28.insert(2, 'Kaolinite_content_square', data_28['Kaolinite_content']**2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_intervals(data_28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prstd, iv_l, iv_u = wls_prediction_std(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.plot(x28, y28, 'o', label=\"data\")\n",
    "#ax.plot(x90, model.predict(x90.reshape(-1,1)), 'b-', label=\"True\")\n",
    "ax.plot(x28, res.fittedvalues, 'r--.', label=\"OLS\")\n",
    "ax.plot(x28, iv_u, 'r--')\n",
    "ax.plot(x28, iv_l, 'r--')\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First sight analysis:**\n",
    "\n",
    "* We can't afford confidence intervals of such a high precission with such a lack of points, specially for high kaolinite contents.\n",
    "* However, this worths a try after adding all the points from the second excel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "* How to deal with small datasets:\\\n",
    "https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89\n",
    "\n",
    "* Feature engineering: \\\n",
    "https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n",
    "\n",
    "* Feature selection: \\\n",
    "https://en.wikipedia.org/wiki/Feature_selection \\\n",
    "https://machinelearningmastery.com/an-introduction-to-feature-selection/ \\\n",
    "https://machinelearningmastery.com/feature-selection-machine-learning-python/ \\\n",
    "https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/ \\\n",
    "https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "\n",
    "* Linear regression with python: \\\n",
    "https://realpython.com/linear-regression-in-python/ \n",
    "\n",
    "* Confidence estimation: \\\n",
    "https://www.puneetarora2000.com/2020/01/data-interpretation-understanding-ols.html \\\n",
    "https://medium.com/@jyotiyadav99111/statistics-how-should-i-interpret-results-of-ols-3bde1ebeec01 \\\n",
    "https://www.statsmodels.org/stable/regression.html \\\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
